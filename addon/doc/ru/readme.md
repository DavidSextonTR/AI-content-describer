# AI Content Describer (Описатель контента с помощью искусственного интеллекта (ИИ)) для NVDA

Это дополнение позволяет получать подробные описания для изображений, элементов управления пользовательского интерфейса и другого визуально недоступного контента.

Используя многообразные возможности передовых моделей искусственного интеллекта (далее - ИИ) и алгоритмов компьютерного зрения, мы стремимся предоставить лучшие в своём классе описания контента и повысить общую независимость. Более подробную информацию о моделях, лежащих в основе дополнения, можно найти в соответствующем разделе этого документа.

## Функции.


* Описывает объект фокуса, объект навигатора, весь экран и позволяет сделать снимок с камеры.
* Описывает любое изображение, которое было скопировано в буфер обмена, будь то картинка из электронной почты или путь к изображению в проводнике windows
* Подсказывает, расположено ли лицо пользователя в центре кадра, используя алгоритмы компьютерного зрения (не требует платного доступа к API)
* Бесплатное использование по умолчанию, при желании можно добавить свой собственный API-ключ для получения дополнительных моделей.
* Поддерживает множество поставщиков моделей ИИ (GPT4 от OpenAI, Gemini от Google, Pixtral Large от Mistral, Claude 3 от Anthropic, Ollama и llama.cpp)
* Поддерживает широкий спектр форматов изображений, включая PNG (.png), JPEG (.jpeg и .jpg), WEBP (.webp) и неанимированный GIF (.gif)
* Опционально кэширует ответы для сохранения квоты API
* Для расширенного использования настраивайте промпты (подсказки) и количество токенов, чтобы адаптировать информацию к вашим потребностям.
* Задавайте дополнительные вопросы и прикрепляйте дополнительные изображения
* Рендеринг в формате Markdown для лёгкого доступа к структурированной информации (просто вставьте, например, "ответить в формате Markdown" в конец подсказки)

## Пример использования

У этого проекта было несколько основных мотивов.

NVDA способна выполнять оптическое распознавание символов (OCR) из коробки, что меняет правила игры. Если вы пытаетесь получить текст из изображения или PDF-документа, это то, что вам нужно.

Однако OCR способен анализировать только те данные, которые *могут* быть текстом. Он не в состоянии учесть контекст, объекты и отношения, переданные в этих изображениях. А интернет полон ими. Логотипы, портреты, мемы, иконки, графики, диаграммы, гистограммы и линейные графики... Да что угодно. Они повсюду, и, как правило, не в том формате, в котором их могут интерпретировать пользователи скринридеров.
До недавнего времени авторы контента неукоснительно предоставляли альтернативные текстовые описания. Хотя это по-прежнему необходимо, трудно изменить тот факт, что высокий стандарт качества является исключением, а не правилом.

Теперь возможности практически безграничны. Вы можете:

* Визуализировать рабочий стол или конкретное окно, чтобы понять расположение значков при обучении.
* Получить подробную информацию о состоянии игр, виртуальных машин и т. д., когда звука недостаточно или он недоступен.
* Понять, что отображается на графике.
* Демистифицировать скриншоты или общий доступ к экрану в Zoom или Microsoft Teams.
* Убедиться, что ваше лицо чётко смотрит в камеру, а фон имеет профессиональный вид, прежде чем записывать видео или участвовать в онлайн-совещаниях.

## Модели

* [GPT4 vision](https://platform.openai.com/docs/guides/vision)
* [Google Gemini pro vision](https://blog.google/technology/ai/google-gemini-ai/), включая последние модели 1.5 Flash, 1.5 Flash 8B, Flash 2.0 и Flash 2.0 Lite Preview.
* [Claude 3 (Haiku, Sonett и Opus)](https://docs.anthropic.com/claude/docs/vision)
* [Pixtral Large](https://mistral.ai/en/news/pixtral-large)
* [Ollama (нестабильный)](https://ollama.com/)
* [llama.cpp (крайне нестабилен и медлителен в зависимости от вашего оборудования, проверен на работу с моделями llava-v1.5/1.6, BakLLaVA, Obsidian и MobileVLM 1.7B/3B)](https://github.com/ggerganov/llama.cpp)

Следуйте инструкциям, приведённым ниже, чтобы заставить каждую из них работать.

## Начало работы

Загрузите последнюю версию дополнения по [этой ссылке](https://github.com/cartertemm/AI-content-describer/releases/latest/). Нажмите на файл на компьютере с установленной NVDA.

Начиная с версии 2025.06.05, использование GPT4 является бесплатным благодаря щедрости сообщества PollinationsAI.

Если у вас есть ресурсы и интерес к изучению дополнительных моделей, вы всегда можете использовать свой собственный API-ключ и уменьшить количество запросов к их серверам. Если у вас нет такой возможности, переходите к разделу `Использование` этого документа.

Следуйте приведённым ниже инструкциям, чтобы получить API-ключ от поддерживаемого поставщика.
Если вы не уверены, какой из них использовать, то, по общему мнению разработчиков и тестеровщиков этого дополнения, Gemini в настоящее время предлагает более приемлемые цены, в то время как OpenAI, похоже, обеспечивает более высокую степень точности. Claude 3 haiku - самый дешёвый и быстрый вариант, но его качество оставляет желать лучшего.
Конечно, эти результаты сильно зависят от поставленной задачи, поэтому мы рекомендуем поэкспериментировать с различными моделями и подсказками, чтобы найти то, что работает лучше всего.

### Получение ключа API от OpenAI:

1. Перейдите на [сайт OpenAI](https://platform.openai.com/account/api-keys).
2. Если у вас ещё нет учётной записи, создайте её. Если есть, войдите в систему.
3. На странице  API keys (API-ключи) нажмите create a new secret key (создать новый секретный ключ). Скопируйте его в буфер обмена.
4. Пополните счёт на сумму не менее 1 доллара.
5. В диалоге настроек NVDA перейдите в категорию "Описатель контента с помощью ИИ", выберите "управление моделями (alt+m)", выберите "GPT4 Vision" в качестве поставщика, при помощи клавиши tab перейдите в поле API-ключ и вставьте сюда только что созданный секретный ключ.

На момент написания документации OpenAI выдаёт новым аккаунтам разработчиков кредиты , которые можно использовать в течение трёх месяцев, после чего они сгорают. По истечении этого срока вам придётся покупать кредиты. Обычно их использование не превышает $5,00 в месяц. Для сравнения, оригинальная версия этого дополнения была разработана за сумму чуть меньше доллара. Вы всегда можете войти в свой аккаунт OpenAI и нажать на "usage" ("Использование"), чтобы узнать свою квоту.

### Получение ключа API от Google

1. Сначала вам нужно создать проект рабочего пространства Google, перейдя по [этой ссылке](https://console.cloud.google.com/projectcreate). Убедитесь, что вы вошли в свой аккаунт Google.
2. Создайте имя длиной от четырёх до тридцати символов, например "gemini" или "NVDA add-on".
3. Перейдите по [этой ссылке](https://makersuite.google.com/app/apikey)
4. Нажмите "Создать ключ API".
5. В диалоге настроек NVDA перейдите в категорию "Описатель контента с помощью ИИ", затем выберите "управление моделями (alt+m)", выберите "Google Gemini" в качестве поставщика, перейдите в поле API-ключ и вставьте сюда только что созданный ключ.

### Получение ключа API от Anthropic

1. Войдите в [консоль Anthropic] (https://console.anthropic.com/login).
2. Нажмите на свой профиль -> API-ключи.
3. Нажмите кнопку Создать ключ.
4. Введите имя ключа, например "AIContentDescriber", затем нажмите "Создать ключ" и скопируйте появившееся значение. Это то, что вы вставите в поле API-ключ в категории "Описатель контента с помощью ИИ" в диалоге настроек NVDA -> управление моделями -> Clod 3.
5. Если вы ещё не сделали этого, приобретите кредиты на сумму не менее $5 на странице планы по адресу https://console.anthropic.com/settings/plans.

### Получение ключа API от Mistral

1. Войдите или создайте учётную запись MistralAI, перейдя на страницу [MistralAI login page] (https://auth.mistral.ai/ui/login).
2. Если вы создаёте или входите в учётную запись впервые, добавьте рабочее пространство в соответствии с запросом, указав имя и приняв условия и положения.
3. После входа в систему выберите в меню пункт «API-ключи».
4. Нажмите кнопку «Создать новый ключ» и скопируйте его в буфер обмена. Это значение вы вставите в поле API-ключ в категории "Описатель контента с помощью ИИ" диалога настроек NVDA -> управление моделями -> Pixtral.
5. Пополните счёт, если это необходимо.

### Настройка Ollama

В настоящее время это предпочтительный вариант для локальной установки.

Хотя интеграция Ollama была протестирована более тщательно, чем llama.cpp, она всё ещё менее стабильна, чем обращение к API, и известно, что в некоторых конфигурациях она ведёт себя странно, вплоть до сбоев на компьютерах, не обладающих необходимыми техническими характеристиками.
Как минимум, если вы пробуете это в первый раз, сохраните все документы и всё важное перед тем, как продолжить, на случай, если это произойдёт с вами.

Начните с того, что убедитесь, что вы можете взаимодействовать с выбранной вами моделью с поддержкой компьютерного зрения с помощью интерфейса командной строки. Для этого необходимо выполнить следующие шаги:

1. Загрузите установочный файл Ollama для Windows со страницы [Ollama downloads](https://ollama.com/download).
2. Запустите этот установочный файл. Он подхватит все зависимости, которые потребуются вашему компьютеру.
3. Найдите модель, которую вы хотите использовать. Список можно найти на ollama.com -> models -> vision, или [здесь](https://ollama.com/search?c=vision).
4. Загрузите и запустите эту модель, открыв командную строку и набрав `ollama run [имя_модели]`, разумеется, заменив "[имя_модели]" на ту, которую вы выбрали в шаге 3. Например, `ollama run llama3.2-vision`.
5. Если процесс завершился успешно, вы окажетесь в интерактивной оболочке, своеобразный локальный (и ограниченный) ChatGPT, в которой можно вводить запросы и получать ответы от модели. Протестируйте её, спросив что-нибудь (что угодно), чтобы проверить, работает ли она, а затем введите "/bye", чтобы выйти из этого интерфейса.
6. Вернувшись в окно консоли, введите `ollama list`. В первом столбце будет указано имя типа "llama3.2-vision:latest".
7. Перейдите в настройки AI Content Describer -> управление моделями -> Ollama. В поле имя модели введите это значение и нажмите OK -> OK. Всё готово! Переключитесь на Ollama в подменю моделей, и через некоторое время она должна заработать.

### Настройка llama.cpp

В настоящее время этот поставщик имеет некоторые ошибки, и ваш пробег может быть очень большим. Его могут использовать только опытные пользователи, заинтересованные в запуске локальных самодостаточных моделей и имеющие соответствующее оборудование.

1. Загрузите llama.cpp. На момент написания этой статьи, этот [pull request](https://github.com/ggerganov/llama.cpp/pull/5882) удаляет мультимодальные возможности, поэтому вы захотите использовать [последнюю версию с поддержкой этого](https://github.com/ggerganov/llama.cpp/releases/tag/b2356).
Если вы работаете на графическом адаптере Nvidia с поддержкой CUDA, загрузите эти предварительно собранные двоичные файлы:
[llama-b2356-bin-win-cublas-cu12.2.0-x64.zip](https://github.com/ggerganov/llama.cpp/releases/download/b2356/llama-b2356-bin-win-cublas-cu12.2.0-x64.zip) и [cudart-llama-bin-win-cu12.2.0-x64.zip](https://github.com/ggerganov/llama.cpp/releases/download/b2356/cudart-llama-bin-win-cu12.2.0-x64.zip).
Шаги по работе с другим графическим адаптером не входят в эту тему, но могут быть найдены в readme к llama.cpp.
2. Распакуйте оба файла в одну папку.
3. Найдите в Huggingface квантованные форматы моделей, которые вы хотите использовать. Для LLaVA 1.6 Vicuna 7B: [llava-v1.6-vicuna-7b.Q4_K_M.gguf](https://huggingface.co/cjpais/llava-v1.6-vicuna-7b-gguf/blob/main/llava-v1.6-vicuna-7b.Q4_K_M.gguf) и [mmproj-model-f16.gguf](https://huggingface.co/cjpais/llava-v1.6-vicuna-7b-gguf/blob/main/mmproj-model-f16.gguf).
4. Поместите эти файлы в папку с остальными двоичными файлами llama.cpp.
5. Из командной строки запустите двоичный файл сервера llava.cpp, передав ему файлы .gguf для модели и мультимодального проектора (как показано ниже):
`server.exe -m llava-v1.6-vicuna-7b.Q4_K_M.gguf --mmproj mmproj-model-f16.gguf`.
6. В диалоговом окне настроек NVDA прокрутите вниз до категории AI Content Describer, затем выберите "manage models (alt+m)", выберите "llama.cpp" в качестве провайдера, сделайте закладку в поле base URL и введите конечную точку, показанную в консоли (по умолчанию "http://localhost:8080").
7. В качестве альтернативы вы можете пропустить некоторые из этих шагов и запустить llama.cpp на удаленном сервере с более высокими характеристиками, чем у вашей локальной машины, а затем ввести эту конечную точку.

## Использование

По умолчанию привязаны пять горячих клавиш:

* NVDA+shift+i: Вызывает меню, в котором предлагается описать с помощью ИИ текущий фокус, объект навигатора, изображение с камеры или весь экран.
* NVDA+shift+u: Описать содержимое текущего объекта навигатора с помощью ИИ.
* NVDA+shift+y: Описать изображение или изображение по пути к файлу  в буфере обмена с помощью ИИ.
* NVDA+shift+j: Описать положение вашего лица в кадре выбранной камеры. Если подключено несколько камер, перейдите в меню описателя содержимого AI (NVDA+shift+i) и выберите ту, которую хотите использовать, с помощью пункта "выбрать камеру" в подменю распознавания лиц.
* NVDA+shift+c: Открыть диалог беседы с ИИ, чтобы задать последующие вопросы.

Три жеста не привязаны:

* Описать содержимое текущего элемента в фокусе с помощью ИИ.
* Сделать снимок экрана, а затем описать его с помощью ИИ.
* Сделать снимок выбранной камерой, а затем описать его с помощью ИИ.

Не стесняйтесь настраивать их в любое время из диалога  жестов ввода.

### Продолжение описания

Иногда ответ, полученный от ИИ, оказывается недостаточным. Возможно, изображение низкого качества, неполное или содержит ненужные детали. Возможно, вы хотите сосредоточиться только на определённом участке или сделать более чёткий снимок без потери контекста.
Получив описание, вы можете нажать NVDA+shift+c или выбрать "Продолжить предыдущее описание" в контекстном меню AI Content Describer (NVDA+shift+i). По умолчанию фокус устанавливается на поле сообщения.
Чтобы добавить дополнительное изображение, просто держите окно беседы открытым и используйте дополнение, как обычно. При получении изображения (с камеры, элемента управления, скриншота и т. д.) вас спросят, хотите ли вы прикрепить его к текущей сессии или начать новую.

## Создание дополнения

Для создания пакета дополнения из исходного кода вам потребуется:

* дистрибутив Python (рекомендуется версия 3.7 или более поздняя). Установщики для Windows можно найти на [Python Website](https://www.python.org). Обратите внимание, что в настоящее время для подготовки исходного кода NVDA и включённых в него сторонних модулей требуется 32-битная версия Python 3.7.
* Scons - [Сайт](https://www.scons.org/) - версия 4.3.0 или более поздняя. Вы можете установить его через PIP. `pip install scons`
* Markdown 3.3.0 или более поздней версии. `pip install markdown`.

Затем откройте выбранный вами терминал:

```
git clone https://github.com/cartertemm/AI-content-describer.git
cd AI-content-describer
scons
```

После завершения выполнения команды `scons` в корень репозитория будет помещён файл *.nvda-addon, готовый к тестированию и выпуску.

Если вы добавите дополнительные строки, которые необходимо перевести, важно перестроить файл .pot следующим образом:

```
scons pot
```

## Как перевести?

На компьютере под управлением ОС Windows:

* скачайте [poedit](https://poedit.net/). Это программа, которую вы будете использовать для перевода каждого сообщения с английского.
* скачайте файл .pot со всеми строками [здесь](https://raw.githubusercontent.com/cartertemm/AI-content-describer/main/AIContentDescriber.pot)
* Откройте файл, который вы только что скачали, в программе poedit. В появившемся окне нажмите "Создать новый перевод" и выберите целевой язык.
* Переведите содержимое исходного текста на язык перевода, а затем вставьте его в поле перевода. Для получения дополнительной помощи нажмите элемент списка -> вхождения кода, затем поднимитесь на строку вверх и прочитайте комментарий, начинающийся с "# Translators: " ("# Переводчики: "). Эти комментарии дополнительно доступны в одном месте в файле .pot.
* Когда всё будет готово, нажмите файл -> сохранить или нажмите ctrl+s, затем выберите место, где будут храниться новые файлы .mo и .po. Эти файлы следует отправить мне по электронной почте или прикрепить в запросе на исправление.
* Переведите содержимое readme.md (этот файл). Не забудьте отправить и его!

## Вклад

Вклад высоко ценится и будет отмечен. Никто и ничто не будет забыто.

Над дополнением работали следующие люди.

* [Mazen](https://github.com/mzanm)
* [Kostenkov-2021](https://github.com/Kostenkov-2021)
* [nidza07](https://github.com/nidza07)
* [Heorhii Halas](nvda.translation.uk@gmail.com)
* [Umut Korkmaz](umutkork@gmail.com): Turkish translation
* [Platinum_Hikari](urbain_onces.0r@icloud.com): French translation
* [Lukas](https://4sensegaming.cz): Czech translation
* [Michaela](https://technologiebezzraku.sk): Slovak translation

Столкнулись с проблемой? Отправьте её в [issue tracker](https://github.com/cartertemm/AI-content-describer/issues).

Есть предложение по новой функции? Создайте тикет, и мы сможем обсудить его реализацию. Pull-запросы без связанных с ними проблем будут рассмотрены, но, скорее всего, займут больше времени, особенно если я решу, что новое исправление или функциональность должны работать не так, как было предложено.

Переводы приветствуются с распростёртыми объятиями.

Если у вас нет Github или вы предпочитаете не использовать его, вы можете [написать мне письмо](mailto:cartertemm@gmail.com) - cartertemm (at) gmail (dot) com.

Спасибо за поддержку!
