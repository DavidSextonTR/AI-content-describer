# Описувач ввмісту за допомогою ШІ для NVDA

Цей додаток дозволяє отримати докладні описи для зображень та іншого візуально недоступного вмісту.

Використовуючи мультимодальні можливості великої мовної моделі GPT-4, ми прагнемо забезпечити найкращі у своєму класі описи вмісту. Для отримання додаткової інформації про базову модель зверніться до [GPT-4V](https://openai.com/research/gpt-4v-system-card).

## Особливості

* Опис об'єкта у фокусі, об'єкта навігатора, всього екрана або фотографії з вебкамери
* Опис будь-якого зображення, яке було скопійовано до буфера обміну — чи то зображення з електронного листа, чи шлях у провіднику Windows
* Визначає, чи обличчя користувача розташоване в центрі кадру, використовуючи алгоритми комп'ютерного зору (не потребує доступу до платного API).
* Підтримує кілька постачальників (GPT4 OpenAI, Gemini Google, Mistral Pixtral Large, Anthropic Claude 3 Ollama та llama.cpp)
* Підтримує широкий спектр форматів, включаючи PNG (.png), JPEG (.jpeg і .jpg), WEBP (.webp) і неанімований GIF (.gif)
* Додатково кешує відповіді, щоб зберегти ліміт API
* Для розширеного використання налаштуйте підказку та кількість токенів, щоб адаптувати інформацію до ваших потреб
* Відтворення Markdown для легкого доступу до структурованої інформації (просто вставте, наприклад, «відповідай в Markdown» у кінці підказки)

## Варіант використання

За цим проектом стояло кілька основних мотивів.

NVDA початково здатна виконувати оптичне розпізнавання символів (OCR), що змінює правила гри. Якщо ви намагаєтеся витягти текст із зображення або PDF-документа, це те, що вам потрібно.

Однак OCR здатний аналізувати лише ті дані, які «можуть» бути текстом. Він не здатен врахувати контекст, об'єкти та взаємозв'язки, передані в цих зображеннях. А в інтернеті їх повно. Логотипи, портрети, меми, іконки, графіки, діаграми, гістограми... Все, що завгодно. Вони всюди, і, як правило, не в тому форматі, який користувачі екранних читачів можуть інтерпретувати.
Донедавна існувала непохитна довіра до авторів вмісту, які надавали альтернативні текстові описи. Хоча це все ще є обов'язковим, важко змінити той факт, що високий стандарт якості є скоріше винятком, ніж правилом.

Тепер можливості майже безмежні. Ви можете:

* Візуалізувати робочий стіл або конкретне вікно, щоб зрозуміти розміщення іконок під час навчання інших
* Отримати детальну інформацію про стан ігор, віртуальних машин тощо, коли звук недостатній або недоступний
* Зрозуміти, що відображається на графіку
* Розпізнати знімки екрана
* Переконатись, що ваше обличчя чітко дивиться на камеру, перш ніж записувати відео або брати участь в онлайн-зустрічі

## Моделі

* [GPT4 vision](https://platform.openai.com/docs/guides/vision)
* [Google Gemini pro vision](https://blog.google/technology/ai/google-gemini-ai/)
* [Claude 3 (Haiku, Sonett, and Opus)](https://docs.anthropic.com/claude/docs/vision)
* [Pixtral Large](https://mistral.ai/en/news/pixtral-large)
* [Ollama (нестабільний)](https://ollama.com/)
* [llama.cpp (надзвичайно нестабільний і повільний залежно від вашого апаратного забезпечення, перевірено на роботу з моделями llava-v1.5/1.6, BakLLaVA, Obsidian і MobileVLM 1.7B/3B)](https://github.com/ggerganov /llama.cpp)

Дотримуйтеся наведених нижче інструкцій, щоб кожна з них працювала.

## Початок роботи

Завантажте останню версію додатка за [цим посиланням](https://github.com/cartertemm/AI-content-describer/releases/latest/). Клацніть файл на комп’ютері з інстальованою NVDA, а потім виконайте наведені нижче інструкції, щоб отримати ключ API від підтримуваного постачальника:
Якщо ви не впевнені, якого постачальника  використовувати, розробники та тестувальники цього додатка погоджуються, що Gemini наразі пропонує більш прийнятну ціну, тоді як OpenAI, схоже, забезпечує вищий ступінь точності. Claude 3 haiku є найдешевшим і найшвидшим варіантом, але якість його не дуже висока.
Звісно, ці результати суттєво залежать від поставленого завдання, тому ми рекомендуємо експериментувати з різними моделями і підказками, щоб знайти найкращий варіант.

### Отримання ключа API від OpenAI:

1. Перейдіть на сторінку [https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys)
2. Якщо у вас ще немає облікового запису, створіть його. Якщо маєте, увійдіть.
3. На сторінці ключів API натисніть (Створити новий секретний ключ). Скопіюйте його в буфер обміну.
4. Поповніть рахунок принаймні на 1 дол
5. У діалозі налаштувань NVDA прокрутіть вниз до категорії «Описувач вмісту за допомогою ШІ», потім перейдіть до поля Ключ API і вставте туди щойно згенерований ключ.

На момент написання цієї статті OpenAI видає кредити на нові акаунти розробників, які можна використовувати протягом трьох місяців, після чого вони втрачаються. Після цього періоду вам потрібно буде придбати кредити. Звичайне використання не повинно перевищувати $5.00 на місяць. Для порівняння, оригінальна версія цього додатка була розроблена за трохи менше долара. Ви завжди можете увійти до свого облікового запису OpenAI і натиснути на "використання", щоб дізнатись свій ліміт.

### Отримання ключа API від Google

1. Спочатку вам потрібно створити проект у робочому просторі Google, перейшовши за посиланням нижче. Переконайтеся, що ви увійшли до свого облікового запису. [https://console.cloud.google.com/projectcreate](https://console.cloud.google.com/projectcreate)
2. Створіть назву від чотирьох до тридцяти символів, як-от «gemini» або «NVDA add-on»
3. Перейдіть за цією URL-адресою: [https://makersuite.google.com/app/apikey](https://makersuite.google.com/app/apikey)
4. Натисніть «створити ключ API»
5. У  діалозі налаштувань NVDA прокрутіть вниз до категорії Описувач вмісту за допомогою ШІ, потім виберіть "керувати моделями (alt+m)", виберіть "Google Gemini" як постачальника, перейдіть до поля "Ключ API" і вставте сюди щойно згенерований ключ.

### Отримання ключа API від Anthropic

1. Увійдіть до [Anthropic console](https://console.anthropic.com/login).
2. Натисніть на свій профіль -> Ключі API.
3. Натисніть Створити ключ.
4. Введіть назву для ключа, наприклад "AIContentDescriber", потім натисніть "Створити ключ" і скопіюйте значення, що з'явиться. Це те, що ви вставите у поле ключа API під категорією Описувач вмісту за допомогою ШІ діалогу налаштувань NVDA -> керувати моделями -> Клод 3.
5. Якщо ви ще цього не зробили, придбайте кредитів щонайменше на 5 доларів США на сторінці планів за адресою [https://console.anthropic.com/settings/plans](https://console.anthropic.com/settings/plans).

### Отримання ключа API від Mistral

1. Увійдіть або створіть обліковий запис MistralAI, перейшовши на [сторінку входу в MistralAI](https://auth.mistral.ai/ui/login).
2. Якщо ви створюєте обліковий запис або входите в нього вперше, додайте робочу область відповідно до запиту, вказавши ім'я та прийнявши умови та положення.
3. Після входу в систему виберіть «API ключі» з меню.
4. Натисніть «створити новий ключ» і скопіюйте його в буфер обміну. Це значення буде вставлено у поле «Ключ API» в категорії «Описувач вмісту за допомогою ШІ» у діалозі налаштувань NVDA -> Керування моделями -> Pixtral.
5. Поповніть рахунок, якщо це необхідно.

### Налаштування Ollama

Наразі це найкращий варіант для локального налаштування.

Хоча інтеграція Ollama була протестована більш широко, ніж llama.cpp, вона все ще менш стабільна, ніж виклик API, і, як відомо, поводиться дивно в деяких конфігураціях, аж до збоїв на комп'ютерах, які не мають необхідних специфікацій
Щонайменше, коли пробуєте це вперше, збережіть усі документи та важливі дані перед тим, як продовжити, щоб бути готовими у випадку, якщо це станеться з вами.

Почніть з перевірки можливості взаємодії з вашою моделлю, здатною до технічного зору, використовуючи інтерфейс командного рядка. Кроки для цього такі:

1. Завантажте інсталяційний файл Ollama для Windows зі сторінки [Завантаження Ollama](https://ollama.com/download).
2. Запустіть цей файл налаштування. Він автоматично завантажить усі залежності, необхідні для вашого компьютера.
3. Знайдіть модель, яку ви хочете використовувати. Список можна знайти на ollama.com -> models -> vision, або [безпосередньо тут](https://ollama.com/search?c=vision).
4. Завантажте і запустіть цю модель, відкривши командний рядок і набравши `ollama run [назва_моделі]`, звичайно, замінивши «[назва_моделі]» на ту, яку ви вибрали на кроці 3. Наприклад, `ollama run llama3.2-vision`.
5. Якщо процес завершився успішно, ви потрапите в інтерактивну оболонку, де зможете вводити запити та отримувати відповіді від моделі, подібно до локалізованого (та обмеженого) ChatGPT. Перевірте це, задавши будь-яке питання, щоб перевірити, чи працює воно, а потім введіть "/bye", щоб вийти з цього інтерфейсу.
6. Повернувшись до діалогу консолі, введіть `ollama list`. У першому стовпчику буде вказано назву на зразок «llama3.2-vision:latest».
7. Перейдіть до налаштувань Описувач вмісту за допомогою ШІ -> керувати моделями -> Ollama. У полі «Назва моделі» введіть це значення і натисніть «ОК» -> «ОК». Все готово! Перейдіть на Ollama в підменю моделі, і через деякий час вона має запрацювати.

### Налаштування llama.cpp

Наразі цей провайдер є дещо нестабільним, і ваш досвід може значно варіюватися. Насправді його варто використовувати лише досвідченим користувачам, які зацікавлені у використанні локальних саморозміщених моделей і мають для цього відповідне апаратне забезпечення.

1. Завантажте llama.cpp. На момент написання цієї статті цей [pull request](https://github.com/ggerganov/llama.cpp/pull/5882) вилучає мультимодальні можливості, тому вам краще використовувати [останню версію з підтримкою цього](https://github.com/ggerganov/llama.cpp/releases/tag/b2356).
Якщо ви працюєте на графічному адаптері Nvidia з підтримкою CUDA, завантажте ці готові двійкові файли:
[llama-b2356-bin-win-cublas-cu12.2.0-x64.zip](https://github.com/ggerganov/llama.cpp/releases/download/b2356/llama-b2356-bin-win-cublas-cu12.2.0-x64.zip) та [cudart-llama-bin-win-cu12.2.0-x64.zip](https://github.com/ggerganov/llama.cpp/releases/download/b2356/cudart-llama-bin-win-cu12.2.0-x64.zip)
Кроки для роботи з іншим графічним адаптером винесено за рамки цієї статті, але їх можна знайти у файлі readme llama.cpp.
2. Розпакуйте обидва файли до однієї теки.
3. Знайдіть у Huggingface квантовані формати моделей, які ви хочете використовувати. Для LLaVA 1.6 Vicuna 7B: [llava-v1.6-vicuna-7b.Q4_K_M.gguf](https://huggingface.co/cjpais/llava-v1.6-vicuna-7b-gguf/blob/main/llava-v1.6-vicuna-7b.Q4_K_M.gguf) та [mmproj-model-f16.gguf](https://huggingface.co/cjpais/llava-v1.6-vicuna-7b-gguf/blob/main/mmproj-model-f16.gguf)
4. Покладіть ці файли до теки з рештою двійкових файлів llama.cpp.
5. З командного рядка запустіть двійковий файл сервера llava.cpp, передавши файли .gguf для моделі та мультимодального проектора (як показано нижче):
`server.exe -m llava-v1.6-vicuna-7b.Q4_K_M.gguf --mmproj mmproj-model-f16.gguf`.
6. У  діалозі налаштувань NVDA прокрутіть вниз до категорії Описувач вмісту за допомогою ШІ, потім виберіть "керувати моделями (alt+m)", виберіть "llama.cpp" як постачальника, перейдіть до поля базової URL-адреси і введіть кінцеву точку, показану в консолі (початково "http://localhost:8080").
7. Крім того, ви можете пропустити деякі з цих кроків і запустити llama.cpp на віддаленому сервері з вищими характеристиками, ніж на вашій локальній машині, а потім ввести ту саму кінцеву точку.

## Використання

Початково призначено чотири гарячі клавіші:

* NVDA+shift+i: відкриває спливаюче меню із запитом, чи потрібно описати поточний фокус, об’єкт навігатора чи весь екран за допомогою ШІ.
* NVDA+shift+u: Описати вміст поточного об'єкта навігатора за допомогою ШІ.
* NVDA+shift+y: Описати зображення (або шлях до файлу зображення) у буфері обміну за допомогою ШІ.
* NVDA+shift+j: описати положення вашого обличчя в кадрі вибраної камери. Якщо у вас під’єднано кілька камер, перейдіть до меню описувача вмісту за допомогою ШІ (NVDA+shift+i) і виберіть ту, яку ви бажаєте використати, за допомогою пункту «Виберіть камеру» у підменю визначення обличчя.

Три жести не призначені:

* Описати вміст об'єкта, на якому перебуває фокус, використовуючи ШІ.
* Зробити знімок екрана, а потім описати його за допомогою ШІ.
* Зробити знімок за допомогою вибраної камери, а потім описати його за допомогою ШІ.

Ви можете налаштувати їх у будь-який час за допомогою діалогу «Жести вводу».

## Внески

Усі вони високо оцінені та будуть зараховані.
Над додатком працювали наступні люди.

* [Mazen](https://github.com/mzanm)
* [Костенков-2021](https://github.com/Kostenkov-2021)
* [Nidza07](https://github.com/nidza07)
* [Георгій Галас](nvda.translation.uk@gmail.com)
* [Умут Коркмаз](umutkork@gmail.com)
* [Platinum_Hikari](urbain_onces.0r@icloud.com)

Виникла проблема? Надішліть її в [систему відстеження проблем](https://github.com/cartertemm/AI-content-describer/issues)

Маєте пропозицію щодо нової функції? Створіть для цього також обговорення, і ми зможемо обговорити його реалізацію. Запити без супутніх проблем будуть розглянуті, але, ймовірно, займуть більше часу для всіх, особливо якщо я вирішу, що нове виправлення або функціонал повинен працювати інакше, ніж було запропоновано.

Якщо у вас немає Github або ви не бажаєте ним користуватися, ви можете [написати мені листа](mailto:cartertemm@gmail.com) - cartertemm@gmail.com

Дякую за підтримку!
